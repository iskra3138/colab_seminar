{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Keras MNIST TPU_Serving@GCP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iskra3138/colab_seminar/blob/master/Keras_MNIST_TPU_Serving_GCP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TBsuwHGAv7w4"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "pry0oCQUv7w8",
        "colab": {}
      },
      "source": [
        "# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xqLjB2cy5S7m"
      },
      "source": [
        "## MNIST on TPU (Tensor Processing Unit)<br>or GPU using tf.Keras and tf.data.Dataset\n",
        "<table><tr><td><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/keras-tensorflow-tpu300px.png\"   width=\"300\" alt=\"Keras+Tensorflow+Cloud TPU\"></td></tr></table>\n",
        "\n",
        "\n",
        "## Overview\n",
        "\n",
        "This sample trains an \"MNIST\" handwritten digit \n",
        "recognition model on a GPU or TPU backend using a Keras\n",
        "model. Data are handled using the tf.data.Datset API. This is\n",
        "a very simple sample provided for educational purposes. \n",
        "<u>**Do not expect outstanding TPU performance on a dataset as\n",
        "small as MNIST**.</u>\n",
        "\n",
        "This notebook is hosted on GitHub. To view it in its original repository, after opening the notebook, select **File > View** on GitHub.\n",
        "\n",
        "## Learning objectives\n",
        "\n",
        "In this notebook, you will learn how to:\n",
        "*   Authenticate in Colab to access Google Cloud Storage (GSC)\n",
        "*   Format and prepare a dataset using tf.data.Dataset\n",
        "*   Create convolutional and dense layers using tf.keras.Sequential\n",
        "*   Build a Keras classifier with softmax, cross-entropy, and the adam optimizer\n",
        "*   Run training and validation in Keras using Cloud TPU\n",
        "*   <u>Export a model for serving from ML Engine</u>\n",
        "*   <u>Deploy a trained model to ML Engine</u>\n",
        "*  <u>Test predictions on a deployed model</u>\n",
        "\n",
        "## Instructions\n",
        "\n",
        "<h3><a href=\"https://cloud.google.com/gpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/gpu-hexagon.png\" width=\"50\"></a>  &nbsp;&nbsp;Train on GPU or TPU&nbsp;&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a></h3>\n",
        "\n",
        "  1. Select a GPU or TPU backend (Runtime > Change runtime type) \n",
        "  1. Runtime > Run All <br/>(<u>**Watch out: the \"Colab-only auth\" cell requires user input. <br/>The \"Deploy\" part at the end requires cloud project and bucket configuration.**</u>)\n",
        "\n",
        "<h3><a href=\"https://cloud.google.com/ml-engine/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/mlengine-hexagon.png\" width=\"50\"></a>  &nbsp;&nbsp;Deploy to AI Platform</h3>\n",
        "<u>At the bottom of this notebook you can deploy your trained model to AI Platform for a serverless, autoscaled, REST API experience. You will need a Google Cloud project and a GCS (Google Cloud Storage) bucket for this last part.</u>\n",
        "\n",
        "<font color='red'>TPUs are located in Google Cloud, for optimal performance, they read data directly from Google Cloud Storage.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qpiJj8ym0v0-"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AoilhmYe1b5t",
        "colab": {}
      },
      "source": [
        "import os, re, time, json\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "print(\"Tensorflow version \" + tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "qhdz68Xm3Z4Z",
        "colab": {}
      },
      "source": [
        "#@title visualization utilities [RUN ME]\n",
        "\"\"\"\n",
        "This cell contains helper functions used for visualization\n",
        "and downloads only. You can skip reading it. There is very\n",
        "little useful Keras/Tensorflow code here.\n",
        "\"\"\"\n",
        "\n",
        "# Matplotlib config\n",
        "plt.rc('image', cmap='gray_r')\n",
        "plt.rc('grid', linewidth=0)\n",
        "plt.rc('xtick', top=False, bottom=False, labelsize='large')\n",
        "plt.rc('ytick', left=False, right=False, labelsize='large')\n",
        "plt.rc('axes', facecolor='F8F8F8', titlesize=\"large\", edgecolor='white')\n",
        "plt.rc('text', color='a8151a')\n",
        "plt.rc('figure', facecolor='F0F0F0')# Matplotlib fonts\n",
        "MATPLOTLIB_FONT_DIR = os.path.join(os.path.dirname(plt.__file__), \"mpl-data/fonts/ttf\")\n",
        "\n",
        "# pull a batch from the datasets. This code is not very nice, it gets much better in eager mode (TODO)\n",
        "def dataset_to_numpy_util(training_dataset, validation_dataset, N):\n",
        "  \n",
        "  # get one batch from each: 10000 validation digits, N training digits\n",
        "  batch_train_ds = training_dataset.unbatch().batch(N)\n",
        "  \n",
        "  # eager execution: loop through datasets normally\n",
        "  if tf.executing_eagerly():\n",
        "    for validation_digits, validation_labels in validation_dataset:\n",
        "      validation_digits = validation_digits.numpy()\n",
        "      validation_labels = validation_labels.numpy()\n",
        "      break\n",
        "    for training_digits, training_labels in batch_train_ds:\n",
        "      training_digits = training_digits.numpy()\n",
        "      training_labels = training_labels.numpy()\n",
        "      break\n",
        "    \n",
        "  else:\n",
        "    v_images, v_labels = tf.compat.v1.data.make_one_shot_iterator(validation_dataset).get_next()\n",
        "    t_images, t_labels = tf.compat.v1.data.make_one_shot_iterator(batch_train_ds).get_next()\n",
        "    # Run once, get one batch. Session.run returns numpy results\n",
        "    with tf.Session() as ses:\n",
        "      (validation_digits, validation_labels,\n",
        "       training_digits, training_labels) = ses.run([v_images, v_labels, t_images, t_labels])\n",
        "  \n",
        "  # these were one-hot encoded in the dataset\n",
        "  validation_labels = np.argmax(validation_labels, axis=1)\n",
        "  training_labels = np.argmax(training_labels, axis=1)\n",
        "  \n",
        "  return (training_digits, training_labels,\n",
        "          validation_digits, validation_labels)\n",
        "\n",
        "# create digits from local fonts for testing\n",
        "def create_digits_from_local_fonts(n):\n",
        "  font_labels = []\n",
        "  img = PIL.Image.new('LA', (28*n, 28), color = (0,255)) # format 'LA': black in channel 0, alpha in channel 1\n",
        "  font1 = PIL.ImageFont.truetype(os.path.join(MATPLOTLIB_FONT_DIR, 'DejaVuSansMono-Oblique.ttf'), 25)\n",
        "  font2 = PIL.ImageFont.truetype(os.path.join(MATPLOTLIB_FONT_DIR, 'STIXGeneral.ttf'), 25)\n",
        "  d = PIL.ImageDraw.Draw(img)\n",
        "  for i in range(n):\n",
        "    font_labels.append(i%10)\n",
        "    d.text((7+i*28,0 if i<10 else -4), str(i%10), fill=(255,255), font=font1 if i<10 else font2)\n",
        "  font_digits = np.array(img.getdata(), np.float32)[:,0] / 255.0 # black in channel 0, alpha in channel 1 (discarded)\n",
        "  font_digits = np.reshape(np.stack(np.split(np.reshape(font_digits, [28, 28*n]), n, axis=1), axis=0), [n, 28*28])\n",
        "  return font_digits, font_labels\n",
        "\n",
        "# utility to display a row of digits with their predictions\n",
        "def display_digits(digits, predictions, labels, title, n):\n",
        "  plt.figure(figsize=(13,3))\n",
        "  digits = np.reshape(digits, [n, 28, 28])\n",
        "  digits = np.swapaxes(digits, 0, 1)\n",
        "  digits = np.reshape(digits, [28, 28*n])\n",
        "  plt.yticks([])\n",
        "  plt.xticks([28*x+14 for x in range(n)], predictions)\n",
        "  for i,t in enumerate(plt.gca().xaxis.get_ticklabels()):\n",
        "    if predictions[i] != labels[i]: t.set_color('red') # bad predictions in red\n",
        "  plt.imshow(digits)\n",
        "  plt.grid(None)\n",
        "  plt.title(title)\n",
        "  \n",
        "# utility to display multiple rows of digits, sorted by unrecognized/recognized status\n",
        "def display_top_unrecognized(digits, predictions, labels, n, lines):\n",
        "  idx = np.argsort(predictions==labels) # sort order: unrecognized first\n",
        "  for i in range(lines):\n",
        "    display_digits(digits[idx][i*n:(i+1)*n], predictions[idx][i*n:(i+1)*n], labels[idx][i*n:(i+1)*n],\n",
        "                   \"{} sample validation digits out of {} with bad predictions in red and sorted first\".format(n*lines, len(digits)) if i==0 else \"\", n)\n",
        "    \n",
        "# utility to display training and validation curves\n",
        "def display_training_curves(training, validation, title, subplot):\n",
        "  if subplot%10==1: # set up the subplots on the first call\n",
        "    plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n",
        "    plt.tight_layout()\n",
        "  ax = plt.subplot(subplot)\n",
        "  ax.grid(linewidth=1, color='white')\n",
        "  ax.plot(training)\n",
        "  ax.plot(validation)\n",
        "  ax.set_title('model '+ title)\n",
        "  ax.set_ylabel(title)\n",
        "  ax.set_xlabel('epoch')\n",
        "  ax.legend(['train', 'valid.'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VE6zp3q_HNTi"
      },
      "source": [
        "*(you can double-click on collapsed cells to view the non-essential code inside)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Lzd6Qi464PsA"
      },
      "source": [
        "### Colab-only auth for this notebook and the TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "MPx0nvyUnvgT",
        "colab": {}
      },
      "source": [
        "IS_COLAB_BACKEND = 'COLAB_GPU' in os.environ  # this is always set on Colab, the value is 0 or 1 depending on GPU presence\n",
        "if IS_COLAB_BACKEND:\n",
        "  from google.colab import auth\n",
        "  # Authenticates the Colab machine and also the TPU using your\n",
        "  # credentials so that they can access your private GCS buckets.\n",
        "  auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R4jujVYWY9-6"
      },
      "source": [
        "### TPU or GPU detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hd5zB1G7Y9-7",
        "colab": {}
      },
      "source": [
        "# Detect hardware\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
        "except ValueError:\n",
        "  tpu = None\n",
        "  gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
        "    \n",
        "# Select appropriate distribution strategy\n",
        "if tpu:\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(tpu, steps_per_run=128) # Going back and forth between TPU and host is expensive. Better to run 128 batches on the TPU before reporting back.\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])  \n",
        "elif len(gpus) > 1:\n",
        "  strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n",
        "  print('Running on multiple GPUs ', [gpu.name for gpu in gpus])\n",
        "elif len(gpus) == 1:\n",
        "  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "  print('Running on single GPU ', gpus[0].name)\n",
        "else:\n",
        "  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "  print('Running on CPU')\n",
        "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msZ10U0FDgxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.distribute.experimental.TPUStrategy?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Lvo0t7XVIkWZ"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cCpkS9C_H7Tl",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64 * strategy.num_replicas_in_sync # Gobal batch size.\n",
        "# The global batch size will be automatically sharded across all\n",
        "# replicas by the tf.data.Dataset API. A single TPU has 8 cores.\n",
        "# The best practice is to scale the batch size by the number of\n",
        "# replicas (cores). The learning rate should be increased as well.\n",
        "\n",
        "LEARNING_RATE = 0.01\n",
        "LEARNING_RATE_EXP_DECAY = 0.6 if strategy.num_replicas_in_sync == 1 else 0.7\n",
        "# Learning rate computed later as LEARNING_RATE * LEARNING_RATE_EXP_DECAY**epoch\n",
        "# 0.7 decay instead of 0.6 means a slower decay, i.e. a faster learnign rate.\n",
        "\n",
        "training_images_file   = 'gs://mnist-public/train-images-idx3-ubyte'\n",
        "training_labels_file   = 'gs://mnist-public/train-labels-idx1-ubyte'\n",
        "validation_images_file = 'gs://mnist-public/t10k-images-idx3-ubyte'\n",
        "validation_labels_file = 'gs://mnist-public/t10k-labels-idx1-ubyte'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Lz1Zknfk4qCx"
      },
      "source": [
        "### tf.data.Dataset: parse files and prepare training and validation datasets\n",
        "Please read the [best practices for building](https://www.tensorflow.org/guide/performance/datasets) input pipelines with tf.data.Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZE8dgyPC1_6m",
        "colab": {}
      },
      "source": [
        "def read_label(tf_bytestring):\n",
        "    label = tf.io.decode_raw(tf_bytestring, tf.uint8)\n",
        "    label = tf.reshape(label, [])\n",
        "    label = tf.one_hot(label, 10)\n",
        "    return label\n",
        "  \n",
        "def read_image(tf_bytestring):\n",
        "    image = tf.io.decode_raw(tf_bytestring, tf.uint8)\n",
        "    image = tf.cast(image, tf.float32)/255.0\n",
        "    image = tf.reshape(image, [28*28])\n",
        "    return image\n",
        "  \n",
        "def load_dataset(image_file, label_file):\n",
        "    imagedataset = tf.data.FixedLengthRecordDataset(image_file, 28*28, header_bytes=16)\n",
        "    imagedataset = imagedataset.map(read_image, num_parallel_calls=16)\n",
        "    labelsdataset = tf.data.FixedLengthRecordDataset(label_file, 1, header_bytes=8)\n",
        "    labelsdataset = labelsdataset.map(read_label, num_parallel_calls=16)\n",
        "    dataset = tf.data.Dataset.zip((imagedataset, labelsdataset))\n",
        "    return dataset \n",
        "  \n",
        "def get_training_dataset(image_file, label_file, batch_size):\n",
        "    dataset = load_dataset(image_file, label_file)\n",
        "    dataset = dataset.cache()  # this small dataset can be entirely cached in RAM\n",
        "    dataset = dataset.shuffle(5000, reshuffle_each_iteration=True)\n",
        "    dataset = dataset.repeat() # Mandatory for Keras for now\n",
        "    dataset = dataset.batch(batch_size, drop_remainder=True) # drop_remainder is important on TPU, batch size must be fixed\n",
        "    dataset = dataset.prefetch(-1)  # fetch next batches while training on the current one (-1: autotune prefetch buffer size)\n",
        "    return dataset\n",
        "  \n",
        "def get_validation_dataset(image_file, label_file):\n",
        "    dataset = load_dataset(image_file, label_file)\n",
        "    dataset = dataset.cache() # this small dataset can be entirely cached in RAM\n",
        "    dataset = dataset.batch(10000, drop_remainder=True) # 10000 items in eval dataset, all in one batch\n",
        "    dataset = dataset.repeat() # Mandatory for Keras for now\n",
        "    return dataset\n",
        "\n",
        "# instantiate the datasets\n",
        "training_dataset = get_training_dataset(training_images_file, training_labels_file, BATCH_SIZE)\n",
        "validation_dataset = get_validation_dataset(validation_images_file, validation_labels_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_fXo6GuvL3EB"
      },
      "source": [
        "### Let's have a look at the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yZ4tjPKvL2eh",
        "colab": {}
      },
      "source": [
        "N = 24\n",
        "(training_digits, training_labels,\n",
        " validation_digits, validation_labels) = dataset_to_numpy_util(training_dataset, validation_dataset, N)\n",
        "display_digits(training_digits, training_labels, training_labels, \"training digits and their labels\", N)\n",
        "display_digits(validation_digits[:N], validation_labels[:N], validation_labels[:N], \"validation digits and their labels\", N)\n",
        "font_digits, font_labels = create_digits_from_local_fonts(N)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KIc0oqiD40HC"
      },
      "source": [
        "### Keras model: 3 convolutional layers, 2 dense layers\n",
        "If you are not sure what cross-entropy, dropout, softmax or batch-normalization mean, head here for a crash-course: [Tensorflow and deep learning without a PhD](https://github.com/GoogleCloudPlatform/tensorflow-without-a-phd/#featured-code-sample)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "56y8UNFQIVwj",
        "colab": {}
      },
      "source": [
        "# This model trains to 99.4% accuracy in 10 epochs (with a batch size of 64)  \n",
        "\n",
        "def make_model():\n",
        "    model = tf.keras.Sequential(\n",
        "      [\n",
        "        tf.keras.layers.Reshape(input_shape=(28*28,), target_shape=(28, 28, 1), name=\"image\"),\n",
        "\n",
        "        tf.keras.layers.Conv2D(filters=12, kernel_size=3, padding='same', use_bias=False), # no bias necessary before batch norm\n",
        "        tf.keras.layers.BatchNormalization(scale=False, center=True), # no batch norm scaling necessary before \"relu\"\n",
        "        tf.keras.layers.Activation('relu'), # activation after batch norm\n",
        "\n",
        "        tf.keras.layers.Conv2D(filters=24, kernel_size=6, padding='same', use_bias=False, strides=2),\n",
        "        tf.keras.layers.BatchNormalization(scale=False, center=True),\n",
        "        tf.keras.layers.Activation('relu'),\n",
        "\n",
        "        tf.keras.layers.Conv2D(filters=32, kernel_size=6, padding='same', use_bias=False, strides=2),\n",
        "        tf.keras.layers.BatchNormalization(scale=False, center=True),\n",
        "        tf.keras.layers.Activation('relu'),\n",
        "\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(200, use_bias=False),\n",
        "        tf.keras.layers.BatchNormalization(scale=False, center=True),\n",
        "        tf.keras.layers.Activation('relu'),\n",
        "        tf.keras.layers.Dropout(0.4), # Dropout on dense layer only\n",
        "\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "      ])\n",
        "\n",
        "    model.compile(optimizer='adam', # learning rate will be set by LearningRateScheduler\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "    \n",
        "with strategy.scope():\n",
        "    model = make_model()\n",
        "\n",
        "# print model layers\n",
        "model.summary()\n",
        "\n",
        "# set up learning rate decay\n",
        "lr_decay = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: LEARNING_RATE * LEARNING_RATE_EXP_DECAY**epoch,\n",
        "    verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CuhDh8ao8VyB"
      },
      "source": [
        "### Train and validate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TTwH_P-ZJ_xx",
        "colab": {}
      },
      "source": [
        "EPOCHS = 10\n",
        "steps_per_epoch = 60000//BATCH_SIZE  # 60,000 items in this dataset\n",
        "print(\"Steps per epoch: \", steps_per_epoch)\n",
        "  \n",
        "# Little wrinkle: in the present version of Tensorfow (1.14), switching a TPU\n",
        "# between training and evaluation is slow (approx. 10 sec). For small models,\n",
        "# it is recommeneded to run a single eval at the end.\n",
        "history = model.fit(training_dataset,\n",
        "                    steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
        "                    callbacks=[lr_decay])\n",
        "\n",
        "final_stats = model.evaluate(validation_dataset, steps=1)\n",
        "print(\"Validation accuracy: \", final_stats[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9jFVovcUUVs1"
      },
      "source": [
        "### Visualize predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w12OId8Mz7dF",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "#@title 기본 제목 텍스트\n",
        "# recognize digits from local fonts\n",
        "probabilities = model.predict(font_digits, steps=1)\n",
        "predicted_labels = np.argmax(probabilities, axis=1)\n",
        "display_digits(font_digits, predicted_labels, font_labels, \"predictions from local fonts (bad predictions in red)\", N)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j00EzzuVaBZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# recognize validation digits\n",
        "probabilities = model.predict(validation_digits, steps=1)\n",
        "predicted_labels = np.argmax(probabilities, axis=1)\n",
        "display_top_unrecognized(validation_digits, predicted_labels, validation_labels, N, 7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5tzVi39ShrEL"
      },
      "source": [
        "## Deploy the trained model to AI Platform model serving\n",
        "\n",
        "Push your trained model to production on AI Platform for a serverless, autoscaled, REST API experience.\n",
        "\n",
        "You will need a GCS (Google Cloud Storage) bucket and a GCP project for this.\n",
        "Models deployed on AI Platform autoscale to zero if not used. <font color='red'>There will be no AI Platform charges after you are done testing.\n",
        "Google Cloud Storage incurs charges. Empty the bucket after deployment if you want to avoid these. Once the model is deployed, the bucket is not useful anymore.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3Y3ztMY_toCP"
      },
      "source": [
        "### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpWjy1eU2A9Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GCP project ID 확인\n",
        "!gcloud projects list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "iAZAn7yIhqAS",
        "colab": {}
      },
      "source": [
        "PROJECT = \"\" #@param {type:\"string\"}\n",
        "BUCKET = \"gs://\"  #@param {type:\"string\", default:\"jddj\"}\n",
        "NEW_MODEL = True #@param {type:\"boolean\"}\n",
        "MODEL_NAME = \"mnist\" #@param {type:\"string\"}\n",
        "MODEL_VERSION = \"v1\" #@param {type:\"string\"}\n",
        "\n",
        "assert PROJECT, 'For this part, you need a GCP project. Head to http://console.cloud.google.com/ and create one.'\n",
        "assert re.search(r'gs://.+', BUCKET), 'For this part, you need a GCS bucket. Head to http://console.cloud.google.com/storage and create one.'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiU_xLuyrtfo",
        "colab_type": "text"
      },
      "source": [
        "### Bucket을 CLI로 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5zchgUlrsw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# serving되는 모델을 저장할 bucket 생성\n",
        "### option argument가 앞에오고 BUCKET이 제일 뒤로 가야 함\n",
        "!gsutil mb -p {PROJECT} {BUCKET}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3o2YNFusJs2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bucket 목록 확인\n",
        "!gsutil ls -p {PROJECT}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GxQTtjmdIbmN"
      },
      "source": [
        "### Export the model for serving from AI Platform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GOgh7Kb7SzzG",
        "colab": {}
      },
      "source": [
        "# Wrap the model so that we can add a serving function\n",
        "class ExportModel(tf.keras.Model):\n",
        "  def __init__(self, model):\n",
        "    super().__init__(self)\n",
        "    self.model = model\n",
        "\n",
        "  # The serving function performig data pre- and post-processing.\n",
        "  # Pre-processing:  images are received in uint8 format converted\n",
        "  #                  to float32 before being sent to through the model.\n",
        "  # Post-processing: the Keras model outputs digit probabilities. We want\n",
        "  #                  the detected digits. An additional tf.argmax is needed.\n",
        "  # @tf.function turns the code in this function into a Tensorflow graph that\n",
        "  # can be exported. This way, the model itself, as well as its pre- and post-\n",
        "  # processing steps are exported in the SavedModel and deployed in a single step.\n",
        "  @tf.function(input_signature=[tf.TensorSpec([None, 28*28], dtype=tf.uint8)])\n",
        "  def my_serve(self, images):\n",
        "    images = tf.cast(images, tf.float32)/255   # pre-processing\n",
        "    probabilities = self.model(images)          # prediction from model\n",
        "    classes = tf.argmax(probabilities, axis=-1) # post-processing\n",
        "    return {'digits': classes}\n",
        "    \n",
        "# Must copy the model from TPU to CPU to be able to compose them.\n",
        "restored_model = make_model()\n",
        "restored_model.set_weights(model.get_weights()) # this copies the weights from TPU, does nothing on GPU\n",
        "\n",
        "# create the ExportModel and export it to the Tensorflow standard SavedModel format\n",
        "serving_model = ExportModel(restored_model)\n",
        "export_path = os.path.join(BUCKET, 'keras_export', str(time.time()))\n",
        "tf.keras.backend.set_learning_phase(0) # inference only\n",
        "tf.saved_model.save(serving_model, export_path, signatures={'serving_default': serving_model.my_serve})\n",
        "\n",
        "print(\"Model exported to: \", export_path)\n",
        "\n",
        "# Note: in Tensorflow 2.0, it will also be possible to\n",
        "# export to the SavedModel format using model.save():\n",
        "# serving_model.save(export_path, save_format='tf')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zm7cpCRQC8-w",
        "colab": {}
      },
      "source": [
        "# saved_model_cli: a useful too for troubleshooting SavedModels (the tool is part of the Tensorflow installation)\n",
        "!saved_model_cli show --dir {export_path}\n",
        "!saved_model_cli show --dir {export_path} --tag_set serve\n",
        "!saved_model_cli show --dir {export_path} --tag_set serve --signature_def serving_default\n",
        "# A note on naming:\n",
        "# The \"serve\" tag set (i.e. serving functionality) is the only one exported by tf.saved_model.save\n",
        "# All the other names are defined by the user in the fllowing lines of code:\n",
        "#      def myserve(self, images):\n",
        "#                        ******\n",
        "#        return {'digits': classes}\n",
        "#                 ******\n",
        "#      tf.saved_model.save(..., signatures={'serving_default': serving_model.myserve})\n",
        "#                                            ***************"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zy3T3zk0u2J0"
      },
      "source": [
        "### Deploy the model\n",
        "This uses the command-line interface. You can do the same thing through the AI Platform UI at https://console.cloud.google.com/mlengine/models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nGv3ITiGLPL3",
        "colab": {}
      },
      "source": [
        "# Create the model\n",
        "if NEW_MODEL:\n",
        "  !gcloud ai-platform models create {MODEL_NAME} --project={PROJECT} --regions=us-central1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvN_ejyP2ZSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o3QtUowtOAL-",
        "colab": {}
      },
      "source": [
        "# Create a version of this model (you can add --async at the end of the line to make this call non blocking)\n",
        "# Additional config flags are available: https://cloud.google.com/ml-engine/reference/rest/v1/projects.models.versions\n",
        "# You can also deploy a model that is stored locally by providing a --staging-bucket=... parameter\n",
        "!echo \"Deployment takes a couple of minutes. You can watch your deployment here: https://console.cloud.google.com/mlengine/models/{MODEL_NAME}\"\n",
        "TF_VER='1.15'\n",
        "!gcloud ai-platform versions create {MODEL_VERSION} --model={MODEL_NAME} --origin={export_path} --project={PROJECT} --runtime-version={TF_VER} --python-version=3.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jE-k1Zn6kU2Z"
      },
      "source": [
        "### Test the deployed model\n",
        "Your model is now available as a REST API. Let us try to call it. The cells below use the \"gcloud ml-engine\"\n",
        "command line tool but any tool that can send a JSON payload to a REST endpoint will work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zZCt0Ke2QDer",
        "colab": {}
      },
      "source": [
        "# prepare digits to send to online prediction endpoint\n",
        "digits_float32 = np.concatenate((font_digits, validation_digits[:100-N])) # pixel values in [0.0, 1.0] float range\n",
        "digits_uint8 = np.round(digits_float32*255).astype(np.uint8) # pixel values in [0, 255] int range\n",
        "labels = np.concatenate((font_labels, validation_labels[:100-N]))\n",
        "with open(\"digits.json\", \"w\") as f:\n",
        "  for digit in digits_uint8:\n",
        "    # the format for AI Platform online predictions is: one JSON object per line\n",
        "    data = json.dumps({\"images\": digit.tolist()})  # \"images\" because that was the name you gave this parametr in the serving funtion my_serve\n",
        "    f.write(data+'\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n6PqhQ8RQ8bp",
        "colab": {}
      },
      "source": [
        "# Request online predictions from deployed model (REST API) using the \"gcloud ml-engine\" command line.\n",
        "predictions = !gcloud ai-platform predict --model={MODEL_NAME} --json-instances digits.json --project={PROJECT} --version {MODEL_VERSION}\n",
        "print(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0xn4HCzgoHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions_np = np.stack([json.loads(p) for p in predictions[1:]]) # first elemet is the name of the output layer: drop it, parse the rest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExUdqMF9FfCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display_top_unrecognized(digits_float32, predictions_np, labels, N, 100//N)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2a5cGsSTEBQD"
      },
      "source": [
        "## What's next\n",
        "\n",
        "* Learn about [Cloud TPUs](https://cloud.google.com/tpu/docs) that Google designed and optimized specifically to speed up and scale up ML workloads for training and inference and to enable ML engineers and researchers to iterate more quickly.\n",
        "* Explore the range of [Cloud TPU tutorials and Colabs](https://cloud.google.com/tpu/docs/tutorials) to find other examples that can be used when implementing your ML project.\n",
        "\n",
        "On Google Cloud Platform, in addition to GPUs and TPUs available on pre-configured [deep learning VMs](https://cloud.google.com/deep-learning-vm/),  you will find [AutoML](https://cloud.google.com/automl/)*(beta)* for training custom models without writing code and [Cloud ML Engine](https://cloud.google.com/ml-engine/docs/) which will allows you to run parallel trainings and hyperparameter tuning of your custom models on powerful distributed hardware.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SVY1pBg5ydH-"
      },
      "source": [
        "## License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hleIN5-pcr0N"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "author: Martin Gorner<br>\n",
        "twitter: @martin_gorner\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Copyright 2019 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is not an official Google product but sample code provided for an educational purpose\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y3LuGqWp2tUB"
      },
      "source": [
        "# 작업했던 bucket과 생성된 model을 삭제하는 코드 (추가됨)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u2tguG_92tUF"
      },
      "source": [
        "## bucket 삭제"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ReXxlJ1w2tUH",
        "colab": {}
      },
      "source": [
        "# bucket 목록 확인\n",
        "!gsutil ls -p {PROJECT}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kXe99SQ72tUZ",
        "colab": {}
      },
      "source": [
        "!gsutil rm -r {BUCKET} "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8JzKysImF-hj",
        "colab": {}
      },
      "source": [
        "# bucket 삭제됨을 확인\n",
        "!gsutil ls -p {PROJECT}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9jnyyMNt2tUl"
      },
      "source": [
        "### model 삭제"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R8b96bEj2tUo",
        "colab": {}
      },
      "source": [
        "# model list 보기\n",
        "!gcloud ai-platform models list --project={PROJECT}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OapDtle72tUu",
        "colab": {}
      },
      "source": [
        "# version 부터 삭제\n",
        "!gcloud ai-platform versions delete {MODEL_VERSION} --model={MODEL_NAME} --project={PROJECT}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zoXIIHZp2tUz",
        "colab": {}
      },
      "source": [
        "# model list로 version 삭제 확인\n",
        "!gcloud ai-platform models list --project={PROJECT}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vxqtTyQX2tU3",
        "colab": {}
      },
      "source": [
        "# model 삭제\n",
        "!gcloud ai-platform models delete {MODEL_NAME} --project={PROJECT}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "63xpNHdj2tU7",
        "colab": {}
      },
      "source": [
        "# model 삭제 확인\n",
        "!gcloud ai-platform models list --project={PROJECT}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vC211Nba2tVC"
      },
      "source": [
        "## Clean Up\n",
        "Before running the next exercise, run the following cell to terminate the kernel and free memory resources:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z0lC-FkJ2tVF",
        "colab": {}
      },
      "source": [
        "import os, signal\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}