{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoGluon HPO for PyTorch",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNZVJYYhO5Ats7Q2lro2yTE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iskra3138/colab_seminar/blob/master/AutoGluon_HPO_for_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-fmoIiziUP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For getting P100\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJd7i_ZOfUIn",
        "colab_type": "text"
      },
      "source": [
        "# AutoGluon Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbRHwOmUfMMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here we assume CUDA 10.0 is installed.  You should change the number\n",
        "# according to your own CUDA version (e.g. mxnet-cu101 for CUDA 10.1).\n",
        "!pip install --upgrade mxnet-cu100\n",
        "!pip install autogluon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yHwbNVXniIHL"
      },
      "source": [
        "- RESTART RUNTIME 버튼을 누르지 말고 아래 에러 해결을 위한 code cell  하나 더 실행하고 RESTART"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJJcMrUKyRYj",
        "colab_type": "text"
      },
      "source": [
        "- task.fit 실행시 'ValueError: max() arg is an empty sequence' 발생 문제 해결 위해 아래 code 셀 실행\n",
        "  - 출처 :<https://github.com/awslabs/autogluon/issues/163>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9HeUetOx23t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip uninstall -y distributed\n",
        "!pip install distributed\n",
        "!pip install -U ipykernel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDf1fMIKh3Lt",
        "colab_type": "text"
      },
      "source": [
        "- RESTART RUNTIME 버튼을 눌러서 런타임 다시 시작하고 아래 부터 실행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIB8Mk7DeyCf",
        "colab_type": "text"
      },
      "source": [
        "# MNIST Training in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R_TbazCjTaP",
        "colab_type": "text"
      },
      "source": [
        "In this tutorial, we demonstrate how to do Hyperparameter Optimization (HPO) using AutoGluon with PyTorch. AutoGluon is a framework agnostic HPO toolkit, which is compatible with any training code written in python. The PyTorch code used in this tutorial is adapted from this [git repo](https://github.com/kuangliu/pytorch-cifar). In your applications, this code can be replaced with your own PyTorch code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCx-KHOEjU5X",
        "colab_type": "text"
      },
      "source": [
        "Import the packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEHkASq7ep85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm.auto import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4qjMGmle0XA",
        "colab_type": "text"
      },
      "source": [
        "### Start with an MNIST Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZp0xYZQe41w",
        "colab_type": "text"
      },
      "source": [
        "##### Data Transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iyt_ldsjYyt",
        "colab_type": "text"
      },
      "source": [
        "We first apply standard image transforms to our training and validation data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGFj58NJes8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose([\n",
        "   transforms.ToTensor(),\n",
        "   transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# the datasets\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZiaubU8e_AB",
        "colab_type": "text"
      },
      "source": [
        "##### Main Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rUeoO0Kjhng",
        "colab_type": "text"
      },
      "source": [
        "The following train_mnist function represents normal training code a user would write for training on MNIST dataset. Python users typically use an argparser to conveniently change default values. The only additional argument you need to add to your existing python function is a reporter object that is used to store performance achieved under different hyperparameter settings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PVgUeyse8IR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_mnist(args, reporter):\n",
        "    # get variables from args\n",
        "    lr = args.lr\n",
        "    wd = args.wd\n",
        "    epochs = args.epochs\n",
        "    net = args.net\n",
        "    print('lr: {}, wd: {}'.format(lr, wd))\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    # Model\n",
        "    net = net.to(device)\n",
        "\n",
        "    if device == 'cuda':\n",
        "        net = nn.DataParallel(net)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=wd)\n",
        "\n",
        "    # datasets and dataloaders\n",
        "    trainset = torchvision.datasets.MNIST(root='./data', train=True, download=False, transform=transform)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = torchvision.datasets.MNIST(root='./data', train=False, download=False, transform=transform)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Training\n",
        "    def train(epoch):\n",
        "        net.train()\n",
        "        train_loss, correct, total = 0, 0, 0\n",
        "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    def test(epoch):\n",
        "        net.eval()\n",
        "        test_loss, correct, total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = net(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                test_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        acc = 100.*correct/total\n",
        "        reporter(epoch=epoch, accuracy=acc)\n",
        "\n",
        "    for epoch in tqdm(range(0, epochs)):\n",
        "        train(epoch)\n",
        "        test(epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE2jos6PfFgS",
        "colab_type": "text"
      },
      "source": [
        "### AutoGluon HPO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy23kT_djlDd",
        "colab_type": "text"
      },
      "source": [
        "In this section, we cover how to define a searchable network architecture, convert the training function to be searchable, create the scheduler, and then launch the experiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-a72adVfH_3",
        "colab_type": "text"
      },
      "source": [
        "##### Define a Searchable Network Achitecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGeVobafjnwV",
        "colab_type": "text"
      },
      "source": [
        "Let’s define a ‘dynamic’ network with searchable configurations by simply adding a decorator [autogluon.obj()](https://autogluon.mxnet.io/api/autogluon.core.html#autogluon.obj). In this example, we only search two arguments hidden_conv and hidden_fc, which represent the hidden channels in convolutional layer and fully connected layer. More info about searchable space is available at [autogluon.space()](https://autogluon.mxnet.io/api/autogluon.space.html#module-autogluon.space)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isNgc0BbfBAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import autogluon as ag\n",
        "\n",
        "@ag.obj(\n",
        "    hidden_conv=ag.space.Int(6, 12),\n",
        "    hidden_fc=ag.space.Categorical(80, 120, 160),\n",
        ")\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, hidden_conv, hidden_fc):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, hidden_conv, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(hidden_conv, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, hidden_fc)\n",
        "        self.fc2 = nn.Linear(hidden_fc, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVtQ1L0ahUDm",
        "colab_type": "text"
      },
      "source": [
        "##### Convert the Training Function to Be Searchable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYPnFmCvjqiE",
        "colab_type": "text"
      },
      "source": [
        "We can simply add a decorator [autogluon.args()](https://autogluon.mxnet.io/api/autogluon.core.html#autogluon.args) to convert the train_mnist function argument values to be tuned by AutoGluon’s hyperparameter optimizer. In the example below, we specify that the lr argument is a real-value that should be searched on a log-scale in the range 0.01 - 0.2. Before passing lr to your train function, AutoGluon always selects an actual floating point value to assign to lr so you do not need to make any special modifications to your existing code to accommodate the hyperparameter search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRoMIYxifKVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@ag.args(\n",
        "    lr = ag.space.Real(0.01, 0.2, log=True),\n",
        "    wd = ag.space.Real(1e-4, 5e-4, log=True),\n",
        "    net = Net(),\n",
        "    epochs=5,\n",
        ")\n",
        "def ag_train_mnist(args, reporter):\n",
        "    return train_mnist(args, reporter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bsbE0mThYXX",
        "colab_type": "text"
      },
      "source": [
        "##### Create the Scheduler and Launch the Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SYInTldhWng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 가지고 있는 CPU/GPU Device 숫자 맞게 수정\n",
        "myscheduler = ag.scheduler.FIFOScheduler(ag_train_mnist,\n",
        "                                         resource={'num_cpus': 1, 'num_gpus': 1},\n",
        "                                         num_trials=2,\n",
        "                                         time_attr='epoch',\n",
        "                                         reward_attr=\"accuracy\")\n",
        "print(myscheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfhtxt6Zharf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "myscheduler.run()\n",
        "myscheduler.join_jobs()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axV-PU_AjBCS",
        "colab_type": "text"
      },
      "source": [
        "We plot the test accuracy achieved over the course of training under each hyperparameter configuration that AutoGluon tried out (represented as different colors)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Umdqu85MjCgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "myscheduler.get_training_curves(plot=True,use_legend=False)\n",
        "print('The Best Configuration and Accuracy are: {}, {}'.format(myscheduler.get_best_config(),\n",
        "                                                               myscheduler.get_best_reward()))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}